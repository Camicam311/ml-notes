{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "- Learning the parameters: Minimize the cost function.\n",
    "<br/><br/>\n",
    "- Minimizing cost function: via Gradient Descent.\n",
    "<br/><br/>\n",
    "- Regularization:(impose penalty term $\\lambda$ to shrink coefficients to prevent overfitting).\n",
    "    1. <strong>Lasso (L1)</strong>: Can minimize some coefficients to zero which leads to feature selection. LASSO (Least Absolute Shrinkage and Selection Operator).\n",
    "$$\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^{p}|\\beta_{j}|$$    \n",
    "```python\n",
    "# import lasso regression\n",
    "from sklearn.linear_model import Lasso\n",
    "# make a list of parameters for tuning\n",
    "alphas = [0.0001, 0.0005, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50, 100]\n",
    "# create Ridge instance\n",
    "lasso_model = Lasso()\n",
    "Lasso(alpha=alphas)\n",
    "```\n",
    "<br/><br/>\n",
    "    2. <strong>Ridge (L2)</strong>: Shrinks all coefficients with same proportion, almost always outperforms L1.\n",
    "$$\\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^{p}\\beta_{j}^{2}$$\n",
    "```python\n",
    "# import ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "# make a list of parameters for tuning\n",
    "alphas = [0.0001, 0.0005, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50, 100]\n",
    "# create Ridge instance\n",
    "ridge_model = Ridge()\n",
    "Ridge(alpha=alphas)\n",
    "```\n",
    "<br/><br/>\n",
    "    3. <strong>Elastic Net</strong>: Combined L1 and L2 as regularizer. Overcomes limitations of L1 (i.e., given large $p$ and small $n$, L1 selects at most $n$ features before it saturates. If there are highly correlated group of variables, L1 tends to select one out of that group and ignores others) by adding a quadratic part to the pentalty $(\\lVert \\beta \\rVert^2)$:\n",
    "$$\\hat\\beta = argmin_{\\beta}(\\lVert y-X\\beta\\lVert^2+\\lambda_2\\lVert \\beta \\rVert^2+\\lambda_1\\lVert \\beta \\rVert_1)$$\n",
    "Quadratic penalty term makes the loss function strongly convex, and therefore has a unique minimum. L1 and L2 are special cases when $\\lambda_1 = \\lambda, \\lambda_2 = 0$ and $\\lambda_1 = 0, \\lambda_2 = \\lambda$.\n",
    "        - Procedure: For a fixed $\\lambda_2$, find coefficients of L2, and then performs L1 shrinkage.\n",
    "        - Due to the double amount of shrinkage (from L1 and L2), this leads to increased bias and poor predictions. To improve prediction performance, we rescale coefficients by multiplying estimated coefficients by $(1+\\lambda_2).$\n",
    "<br/><br/>\n",
    "        - Choosing best lambdas (i.e. $\\lambda_1,\\lambda_2$): via cross validation .\n",
    "```python\n",
    "# import Elastic Net\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# make a list of parameters for tuning\n",
    "alphas = [0.0001, 0.0005, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50, 100]\n",
    "# create Ridge instance\n",
    "en_model = ElasticNet\n",
    "ElasticNet(alpha=alphas)\n",
    "```\n",
    "<br/><br/>        \n",
    "- Choosing between regularization methods: If variables are highly correlated (most variables are useful), L2. If variables are rather independent (some variables are not important), L1 as it leads to variable selection. If there are more variables then observations and high correlation between parameters, Elastic Net\n",
    "<br/><br/>    \n",
    "- Assumes linear relationship between features and label.\n",
    "<br/><br/>\n",
    "- Can add polynomial and interaction features to add non-linearity.\n",
    "<br/><br/>\n",
    "- Loss functions/Evaluation Metrics:\n",
    "    1. <strong>Mean Absolute Error (MAE)</strong>: Easy to understand and assigns equal weights to errors.\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat y_i|$$\n",
    "    2. <strong>Mean Squared Error (MSE)</strong>: Punishes larger errors, i.e. gives higher weights to larger errors.\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat y_i)^2$$\n",
    "    3. <strong>Root Mean Squared Error (RMSE)</strong>: More popular than MSE since it is interpretable in the y units.\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat y_i)^2}$$\n",
    "\n",
    "<br/><br/>\n",
    "Sources: \n",
    "- https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "- https://github.com/ShuaiW/data-science-question-answer/blob/master/README.md#l1-vs-l2-regularization\n",
    "- https://www.youtube.com/watch?v=1dKRdX9bfIo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Boost\n",
    "- Gradient boost for regression is when Gradient Boost is used to predict value for a numerical feature. This is different from simply using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "- Generalized Linear Model (GLM) for binary classification problems.\n",
    "<br/><br/>\n",
    "- Apply sigmoid function to the outputs of the linear models, resulting in outputs between 0 or 1. If > 0.5, usually 1, and 0 otherwise.\n",
    "<br/><br/>\n",
    "- It is a special case of the softmax function that deals with multi-class problems\n",
    "$$\\sigma(t)=\\frac{e^t}{e^t+1}=\\frac{1}{1+e^{-t}}, t = \\beta_0+\\beta_1x$$\n",
    "![img](assets/sigmoid.png)\n",
    "<br/><br/>\n",
    "- Model Evaluation: <strong>Confusion matrix</strong>\n",
    "    - True Positives (TP)\n",
    "    - True Negatives (TN)\n",
    "    - False Positive (FP, Type I Error)\n",
    "    - False Negative (FN, Type II Error)\n",
    "    - <strong>Accuracy</strong>: (TP + TN) / total\n",
    "    - <strong>Misclassification Rate</strong>: (FP + FN) / total\n",
    "    - <strong>Precision</strong>: The ability to classify an instance as positive, when in reality it is negative. For all the instances that were labeled as positive, what percent is correct?\n",
    "$$\\frac{TP}{TP+FP}=\\frac{terrorists correctly identified}{terrorists correctly identified + people incorrectly classified as terrorists}$$\n",
    "<br/><br/>\n",
    "    - <strong>Recall</strong>: The ability for a classifier to find all the positive instances. For all the instances that were actually positive, what percent is classified correctly?\n",
    "$$\\frac{TP}{TN+FN}=\\frac{terrorists correctly identified}{terrorists correctly identified + terrorists incorrectly labeld as not terrorists}$$\n",
    "<br/><br/>\n",
    "    - <strong>F1-Score</strong>: Weighted harmonic mean of <strong>precision</strong> and <strong>recall</strong>. Ranges between 0 to 1, 1 being the best score. In general, F1-Scores are lower accuracy measures as they embed precision and recall into their computation. As a rult of thumb, the weighted average of F1 should be used to compare classifier models instead of global accuracy.\n",
    "$$F_1 = 2*\\frac{precision*recall}{precision+recall}$$\n",
    "<br/><br/>\n",
    "    - <strong>Support</strong>: Number of actual occurrences of the class in the specified dataset. Imbalanced support in the training set indicates structural weakness in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support does not change between models but instead diagnoses the evaluation procewss.\n",
    "\n",
    "<br/><br/>\n",
    "Sources: \n",
    "- https://github.com/ShuaiW/data-science-question-answer/blob/master/README.md#l1-vs-l2-regularization\n",
    "- https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes (NB)\n",
    "- Supervised Learning Algorithm based on Bayes' theorem:\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "<br/><br/>\n",
    "- Called Naive because it builds on the 'naive' assumption that each feature are independent from each other.\n",
    "<br/><br/>\n",
    "- Can make different assumptions: Gaussian, Bernoulli and Multinomial.\n",
    "    - <strong>Gaussian NB</strong>: Used under the assumption that variables are continuous and normally distributed.\n",
    "$$p(x=v|C_k)=\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}e^{-\\frac{(v-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
    "\n",
    "    - <strong>Multinomial NB</strong>: Assumes that samples represent frequencies of events generated from Multinomial distribution, where $(p_1,...,p_n)$ are the probabilities of $1,...n$ events. A feature factor, $x=(x_1,...,x_n)$ is a histogram with $x_i$ indicating the number of times an event $i$ was observed for an instance. An example of its application is text classification, with events representing the occurrence of a word in a single document (BoW:Bag of Words).\n",
    "$$likelihood = p(x|C_k)=\\frac{(\\sum_ix_i)!}{\\prod_ix_i!}\\prod_ip_{ki}^{x_i}$$\n",
    "    - <strong>Bernoulli NB</strong>: Assumes that features are independent and binary. Also widely used in text classification where binary term occurrence features are used instead of term frequencies.\n",
    "$$likelihood = p(x|C_k)=\\prod_{i=1}^{n}p_{ki}^{x_i}(1-p_{ki})^{1-x_i}$$\n",
    "<br/><br/>    \n",
    "- Works well with text classification (baseline model for text classficiation too).\n",
    "<br/><br/>\n",
    "- Extremely fast compared to other models\n",
    "<br/><br/>\n",
    "$$posterior=\\frac{prior*likelihood}{evidence} $$\n",
    "\n",
    "Sources:\n",
    "- https://github.com/ShuaiW/data-science-question-answer/blob/master/README.md#l1-vs-l2-regularization\n",
    "- https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KNN (K-Nearest Neighbor)\n",
    "- A non-parametric method used for classification and regression.\n",
    "<br/><br/>\n",
    "- KNN Classification: output is a class membership. An object is assigned to the class most common among its k nearest neighbors. If k=1, then the object is assigned to the class of that single neighbor.\n",
    "<br/><br/>\n",
    "- KNN regression: output is the property value for the object. This value is the avg of the values of k nearest neighbors.\n",
    "<br/><br/>\n",
    "- Lazy learning algorithm.\n",
    "<br/><br/>\n",
    "- Given a datapoint, find the K nearest data points using distance metric (i.e. Euclidean). In classification, take majority label of neighbors. In regression, take mean of labels.\n",
    "<br/><br/>\n",
    "- We do not train the model in KNN, instead we simply compute during inference time. This is computationally expensive since each of the test example need to be compared with every training example to compare how close they are.\n",
    "<br/><br/>\n",
    "- There are approx. methods that are faster by partitioning train data into regions (i.e. ANNOY, Approximate Nearest Neighborhors Oh Yeah).\n",
    "<br/><br/>\n",
    "- When k=1 or some other small number of the model is prone to overfitting (high variance). On the other hand, when k = no. data pts or other large number, then the model is prone to underfitting (high bias). See image below:\n",
    "![img](assets/knn.png)\n",
    "<br/><br/>\n",
    "- Steps:\n",
    "    1. Calculate distance from point x (new data point) to all points in your data.\n",
    "    2. Sort the points in your data by increasing distance from x.\n",
    "    3. Predict the majority label of the \"k\" closest points\n",
    "    4. Note: Choosing K will affect what class a new point is assigned to:\n",
    "<br/><br/>\n",
    "- <strong>Elbow Method</strong>: A method to determine optimal value for 'k'.\n",
    "<br/><br/>\n",
    "- Pros and Cons:\n",
    "    - Pros: simple, easy to train / Works with any no. of classes / Easy to add more data / few parameters (i.e. k's and distnace metric)\n",
    "    - Cons: High prediction cost (worse for large data) / Not good with high dimensional data / don't work well with categorical features\n",
    "<br/><br/>\n",
    "- When given unlabeled data without context, useful to classify data.\n",
    "\n",
    "Sources:\n",
    "- https://github.com/ShuaiW/data-science-question-answer/blob/master/README.md#l1-vs-l2-regularization\n",
    "- https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVM (Support Vector Machines)\n",
    "- Can perform linear, nonlinear or outlier detection (unsupervised)\n",
    "<br/><br/>\n",
    "- Large margin classifier: SVM determines decision boundary and sets the boundary to be as far from the closest training point as possible.\n",
    "<br/><br/>\n",
    "- Support Vectors: are the closest training examples since they are the points based on where the decision boundary is drawn.\n",
    "<br/><br/>\n",
    "- Sensitive to <strong>feature scaling</strong> (method used to normalize the range of independent variables or feature data (aka data normalization). \n",
    "<br/><br/>\n",
    "- <strong>Terminologies</strong>:\n",
    "    - <strong>Margin</strong>: Shortest distance between the observations and the threshold (i.e. decision boundary).\n",
    "<br/><br/>\n",
    "    - <strong>Maximal Margin classifier</strong>: Using the threshold that gives the largest margin to make classification.\n",
    "        - Sensitive to outliers in the training set.\n",
    "        - So we choose threshold to allow misclassification (bias & variance trade-off).\n",
    "<br/><br/>\n",
    "    - <strong>Soft Margin</strong>: When we allow misclassification, the distance between the observations and the threshold.\n",
    "        - We use Cross Validation to determine how many misclassifications and observations to allow inside the soft margin to get better classification.\n",
    "<br/><br/>\n",
    "    - <strong>Support Vector Classifier (SVC)</strong>: When we use soft margin to determine the location of the threshold. SV comes from the fact that the observations on the edge and within the soft margin are called Support Vectors.\n",
    "        - Handles outliers and allow misclassification.\n",
    "        - Since they allow misclassification, they can handle overlapping classifications (when data pts overlaps each other.\n",
    "        - In 2D, SVC is a line (1D subspace) and in 3D, SVC is a plane (2D subspace).\n",
    "        - Does not perform well when one class of data points are sandwiched between another class of data points.\n",
    "            - This is where SVM comes into play.\n",
    "![img](assets/svc-weakness.png)            \n",
    "<br/><br/>\n",
    "    - <strong>Support Vector Machines (SVM)</strong>: Adds another axis which are squared values of the original axis where data points lie to classify data on higher dimension by SVC. \n",
    "        - Uses Kernel functions (i.e. Polynomial Kernel with parameter <strong>d</strong>=polynomial degree) to systematically find SVC in higher dimensions.\n",
    "        - We find the optimal value of <strong>d</strong> using Cross Validation.\n",
    "        - Another commonly used Kernel Function: Radial Basis Function (RBF)\n",
    "            - RBF finds SVC in infinite dimensions\n",
    "            - When given new/test data, RBF behaves like a <strong>Weighted Nearest Neighbor</strong> model (closest observations have a lot of influence on how we classify new observation).\n",
    "![img](assets/svm-method.png)   \n",
    "![img](assets/svm-result.png)   \n",
    "<br/><br/>\n",
    "- <strong>Kernel Trick</strong>: Calculating high dimensional relationships without actually transforming the data into higher dimension.\n",
    "    - Reduces the computation complexities of SVM\n",
    "    - Makes calculating relationships in the infinite dimensions possible (RBF).\n",
    "<br/><br/>\n",
    "- Perform <strong>Grid Search</strong> to find optimal parameters and better recall values.\n",
    "<br/><br/>\n",
    "\n",
    "Sources:\n",
    "- https://en.wikipedia.org/wiki/Feature_scaling\n",
    "- https://www.youtube.com/watch?v=efR1C6CvhmE\n",
    "- https://github.com/ShuaiW/data-science-question-answer#knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Tree (DT)\n",
    "- Non-parametric, supervised learning algo\n",
    "<br/><br/>\n",
    "- Given training set, DT divides feature space into regions. For inference, determine which region the test data fall in and take the mean label values (regression) or the majority label value (classification).\n",
    "<br/><br/>\n",
    "- <strong>Construction</strong>: From top to down, selects a variable to split the data where the target variables within each region are as homogenous as possible.\n",
    "    - Two common metrics: gini impurity or information gain\n",
    "    - Steps:\n",
    "        1. Calculate impurity for each columns to determine root node (lowest impurity).\n",
    "        2. If node itself has lowest impurity score, there is no point in splitting and becomes a leaf node.\n",
    "        3. If splitting leads to an improvement, then pick separation with lowest impurity value.\n",
    "<br/><br/>\n",
    "- <strong>Pros</strong>: easy to understand and interpret\n",
    "<br/><br/>\n",
    "- <strong>Cons</strong>: \n",
    "    - May lead to overfitting if depth of the tree is not set\n",
    "    - A small change in the training data might lead to a tree that is significantly different.\n",
    "    - sensitive to training set rotation due to its orthogonal decision boundaries.\n",
    "<br/><br/>\n",
    "- Consists of the <strong>Root Node</strong>, <strong>Internal Nodes</strong> and <strong>Leaf Nodes</strong>\n",
    "\n",
    "\n",
    "Sources:\n",
    "- https://github.com/ShuaiW/data-science-question-answer#knn\n",
    "- https://www.youtube.com/watch?v=7VeUPuFGJHk&t=1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest\n",
    "- Steps:\n",
    "    1. Take bootstrapped sample of the data (repetition is possible).\n",
    "    2. Create decision trees using data from step 1, but only use random subset of columns at each step (when building nodes).\n",
    "    3. Repeat steps 1 and 2 n times\n",
    "    4. Given all the trees, test them on the test set, i.e. given one observation, run it down through all n trees and pick answer with the most votes.\n",
    "<br/><br/>\n",
    "- Usually 1/3 of data (<strong>Out-of-bag data)</strong> will not be included in the bootstrapped dataset. Out-of-bag data will be used to test the model.\n",
    "<br/><br/>\n",
    "- <strong>Out-of-bag Error</strong>: Proportion of out-of-bag samples that were incorrectly classified.\n",
    "<br/><br/>\n",
    "- <strong>Bagging</strong>: Bootstrapping data + using the aggregate to make the decision.\n",
    "<br/><br/>\n",
    "- Can use out-of-bag error to compare the number of subset of columns to choose in step 2 above.\n",
    "    - Usually start with square-root of the number of columns, $\\sqrt{p}$, and try few settings above and below that value.\n",
    "<br/><br/>\n",
    "- <strong>Dealing with missing data</strong>: replace with the most common (mode) value or median value.\n",
    "<br/><br/>\n",
    "- <strong>Refining guess</strong>: Determine which samples are similar. Similar in a sense that they ended up in the same leaf node. Similarity check using proximity matrix (weighted frequency).\n",
    "<br/><br/>\n",
    "- Improve bagging further by adding some randomness. In random forest, only a subset of features are selected at random to construct a tree. Benefits: decorrelates the trees.\n",
    "<br/><br/>\n",
    "- For example, given a dataset, supposed that there is only one very predicative feature and a couple of moderately predicative features. In bagging trees, most of the trees will use the very predicative feature in the top split so the rest of the trees look highly correlated and similar. Taking the average of the many highly correlated results will not lead to a large decrease in variance as opposed to uncorrelated results. In random forest, for each split, we only consider a subset of features and hence reduce variance further by introducing more uncorrelated trees.\n",
    "<br/><br/>\n",
    "- In practice, tuning RF involves having a large number of trees (the more the merrier, but there is alway the issue of computational cost).\n",
    "<br/><br/>\n",
    "- `min_samples_leaf` used (minimum number of samples at the leaf node) to control the tree size and overfitting.\n",
    "<br/><br/>\n",
    "- Important to CV parameters.\n",
    "\n",
    "Sources:\n",
    "- https://github.com/ShuaiW/data-science-question-answer#knn\n",
    "- https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "- https://www.youtube.com/watch?v=nyxTdL_4Q-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Boosting Tree\n",
    "How it works:\n",
    "<br/><br/>\n",
    "It builds on weak learners in an iterative way. In every iteration, a new learner is added, while existing learners remain unchanged. All learners are weighted based on their performance (i.e. accuracy) and afte a weak learner is added, data are weighted again: examples that are misclassified gain more weights, while examples that are correctly classified lose weights. Hence, future weak learners focus more on examples that previous weak learners misclassified.\n",
    "\n",
    "- How it is different from RF:\n",
    "    1. RF grows trees in parallel, while in boosting, it is sequential.\n",
    "    2. RF reduces variance, while boosting reduces errors by reducing bias.\n",
    "<br/><br/>\n",
    "- <strong>Ada Boost</strong> (Adaptive Boosting): (convert set of weak classifiers into strong ones)\n",
    "    - Whereas in RF trees made with no distinct depth, in Ada Boost, trees are usually just a node with 2 learners (forest of stumps).\n",
    "<br/><br/>\n",
    "    - <strong>Stump</strong>: Tree with 1 node and 2 leaves.\n",
    "        - Stumps are 'weak learners' since they only use one variable.\n",
    "    - Whereas in RF all tree's final classification have same equal weights, a forest of stumps in Ada Boost have some stumps with more weights on final classification.\n",
    "<br/><br/>\n",
    "    - In RF, each tree is independent of another (i.e. order does not matter) whiel in Ada Boost, order matters (error from 1st stump affects the following stumps).\n",
    "<br/><br/>\n",
    "    - 3 ideas behind Ada Boost:\n",
    "        1. AdaBoost combines a lot of 'weak' learners to classify.\n",
    "        2. Some stumps weigh more than the others.\n",
    "        3. Each stump is made by taking previous stump's mistakes into account.\n",
    "<br/><br/>\n",
    "    - For the first stump, each sample's weights are equal.\n",
    "<br/><br/>\n",
    "    - Total error of a stump: sum of the weights associated w/ incorrectly classified samples.\n",
    "        - total error $\\in [0,1]$, where 0 = perfect stump and 1 = horrible stump\n",
    "        - Determines the weight of stump:\n",
    "$$\\frac{1}{2}\\log(\\frac{1-tot.error}{tot.error})$$\n",
    "<br/><br/>\n",
    "    - Increase sample weight for samples that incorrectly classified the result.\n",
    "<br/><br/>\n",
    "    - AdaBoost works by putting more weight on difficult to classify samples and less on those already handled well.\n",
    "<br/><br/>\n",
    "    - AdaBoost can be used for both classification and regression problem.\n",
    "<br/><br/>\n",
    "- <strong>XGBoost</strong> (Extreme Gradient Boosting):\n",
    "<br/><br/>\n",
    "    - XGBoost is a tree-based ensembled ML algorithm that uses gradient boosting framework.\n",
    "<br/><br/>\n",
    "       - While artificial neural networks tend to outperform for problems involving unstructured data (images, texts, etc.), tree based methods are considered best-in-class when it comes to small-medium structured data. Following is a picture describing evolution of the tree-based methods:\n",
    "<br/><br/>\n",
    "![img](assets/tree-based-methods.jpeg)   \n",
    "<br/><br/>\n",
    "        - Can be used for regression, classification, ranking and other user-defined prediction problems.\n",
    "<br/><br/>\n",
    "Sources:\n",
    "- https://github.com/ShuaiW/data-science-question-answer\n",
    "- https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\n",
    "- https://www.youtube.com/watch?v=LsK-xG1cLYA&t=748s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes:\n",
    "- <strong>Tuning parameter</strong>: a parameter that is not estimated but guessed (i.e. Ridge Regression (L2) has a tuning parameter, $\\lambda_2$). Use k-fold (i.e. 10) CV to find the best guess for the tuning parameter.\n",
    "<br/><br/>\n",
    "- <strong>Cross Validation</strong>: allows us to compare different ML methods and get a sense of how well they work in practice.\n",
    "<br/><br/>\n",
    "- <strong>Bias</strong>: Inablity for ML method to capture the true relationship\n",
    "<br/><br/>\n",
    "- <strong>Variance</strong>: Difference in fits between datasets.\n",
    "<br/><br/>\n",
    "- <strong>Hyperparameter</strong>: is a characteristic of a model that is external to the model and whose value cannot be estimated from data. Value for the hyperparameter has to be set before the learning process begins. Examples include $c$ in SVM, $k$ in KNN, # of hidden layers in Neural Networks.\n",
    "<br/><br/>\n",
    "- <strong>Parameter</strong>: is an internal characteristic of the model and its value can be estimated from data. Examples include coefficients of Linear model or SVC's in SVM.\n",
    "<br/><br/>\n",
    "- <strong>Grid Search</strong>: Used to find the optimal hyperparameters of a model for the most accurate predictions. Used to find better recall value.\n",
    "    - C parameter: gives you low bias and high variance.\n",
    "        - High value of C: low variance\n",
    "        - Low value of C: low variance and high bias.\n",
    "    - Kernel paramter: Gaussian radial basis /RBF\n",
    "    - Gamma parameter: \n",
    "        - Small gamma = gaussian with large variance. \n",
    "        - Large gamma = high bias and low variance.\n",
    "<br/><br/>\n",
    "- <strong>Baseline performance</strong>: To answer to question: \"What would be the success rate of the model if one were simply guessing?\"\n",
    "<br/><br/>\n",
    "- <strong>Bagging</strong>: General purpose procedure for reducing the variance of ML method.\n",
    "<br/><br/>\n",
    "- <strong>Bootstrapping</strong>:\n",
    "<br/><br/>\n",
    "- <strong>Understanding the evolution of tree-based methods</strong>: Imagine you are a hiring manager interviewing candidates with excellent qualifications. Each step of the evolution of tree-based algorithm can be viewed as a version of the interview process.\n",
    "    1. Decision Tree: Every hiring manager has a set of criteria when hiring a candidate, i.e. education level, number of years of experience, interview performance. Here, decision tree is analogous to a hiring manager interviewing candidates based on the criteria.\n",
    "    2. Bagging: Instead of a single interviewer, there is an interview panel where each interviewer has a vote. Bagging or bootstrap aggregating involves combining inputs from all interviewers for the final decision through a democratic voting process.\n",
    "    3. RF: Bagging based algorithm where only a subset of features are chosen at random. Every interviewer will only test candidates on certain randomly selected criteria.\n",
    "    4. Boosting: Alternative approach where interviewer changes the evaluation criteria based on feedback from the previous interviewer. This 'boosts' the efficiency of the interview process by deploying a more dynamic evaluation process.\n",
    "    5. Gradient Boosting: Special case of boosting where errors are minimized by gradient descent. This is the strategy consulting firms leverage by using case interviews to weed out less qualified candidates.\n",
    "    6. XGBoost: Gradient boosting on steriods. Perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.\n",
    "<br/><br/>\n",
    "- <strong>CART</strong>: Classification And Regression Trees.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e\n",
    "- https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
