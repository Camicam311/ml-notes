{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize vs. Normalize\n",
    "- <strong>Rescaling</strong>: add/subtract a constant and then multiply/divide by constant. Example: converting from Farenheit to Celsius.\n",
    "<br/><br/>\n",
    "- <strong>Normalizing</strong>: a vector means that dividing by the norm of the vector. Also known as rescaling by minimum and range of the vector to make elements between 0 and 1, thus bringing all the columns into a common scale.\n",
    "<br/><br/>\n",
    "- <strong>Standardizing</strong>: a vector means that subtracting a measure of location and dividing by a measure of scale. For instance, consider a vector of values that are generated from a normal distribution, here you subtract the mean and divide by the standard deviation, hence obtaining the \"standard normal\" RV with $\\mu=0$ and $\\sigma=1$\n",
    "<br/><br/>\n",
    "- Questions to be answered:\n",
    "    1. Why should you standardize/normalize/scale your data?\n",
    "    2. How to standardize your numeric attributes to have a 0 mean and 1 std.\n",
    "    3. How to normalize your numeric attributes between the range of 0 and 1 using min/max scalar.\n",
    "    4. How to normalize using robust scalar.\n",
    "    5. When to choose standardization or normalization.\n",
    "<br/><br/>    \n",
    "- Why Standardize or Normalize?\n",
    "    - <strong>Standardization</strong>: standardizing features is important when we compare measurements that have different units. Variables that are measured at different scales do not contribute equally to the analysis and might end up creating a bias.\n",
    "        - For example, A variable that ranges between 0 and 1000 will outweigh a variable that ranges between 0 and 1. Using these variables without standardization will give the variable with the larger range weight of 1000 in the analysis. Transforming the data to comparable scales can prevent this problem. Typical data standardization procedures equalize the range and/or data variability.\n",
    "<br/><br/>    \n",
    "    - <strong>Normalization</strong>: The goal of normalization is to change the values of the numeric columns in the dataset to a common scale, without distorting differences in the range of values. For ML, every dataset does not require normalization. It is required only when features have different ranges.\n",
    "        - For example, consider a data set containing two features, age, and income(x2). Where age ranges from 0–100, while income ranges from 0–100,000 and higher. Income is about 1,000 times larger than age. So, these two features are in very different ranges. When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range.\n",
    "<br/><br/>    \n",
    "- When do you Standardize or Normalize?\n",
    "    - <strong>Normalization</strong>: is used when you do not know the distribution of the data or when you know that the data is not normally distributed. Normalization is useful when you data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as KNN and aritificial neural nets. The following code normalizes data:\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# find mean and std\n",
    "print(data_scaled.mean(axis=0))\n",
    "print(data_scaled.std(axis=0))\n",
    "```\n",
    "<br/><br/>\n",
    "    - <strong>Standardization</strong>: assumes that your data comes from a normal distribution. This does not strictly have to be true, but it is more effective if your data is normal. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a normal distribution, examples include linear regression, logistic regression and linear discriminant analysis. The following code standardizes data:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# find mean and standard deviation\n",
    "print(data_scaled.mean(axis=0))\n",
    "print(data_scaled.std(axis=0))\n",
    "```\n",
    "<br/><br/>\n",
    "- <strong>Robust Scalar </strong>(Scaling to median and quantiles): Scaling using median and quantiles consists of subtracting the median to all the observations and then dividing by the interquartile difference. It scales features using statistics that are robust to outliers. Interquartile difference is the difference between the 75th and the 25th quartile.\n",
    "$$X_{scaled} = \\frac{X-median(X)}{IQR}$$\n",
    "<br/><br/>\n",
    "$$IQR = 75th quartile - 25th quartile$$\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler() \n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# get mean and std\n",
    "print(data_scaled.mean(axis=0))\n",
    "print(data_scaled.std(axis=0))\n",
    "```\n",
    "<br/><br/>\n",
    "Sources:\n",
    "- https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Methods in normalizing data\n",
    "- Normalization Function\n",
    "$$y = \\frac{X - min(X)}{max(X)-min(X)}$$\n",
    "<br/><br/>\n",
    "- Sigmoid Function: As $x$ goes to negative infinity, $f(x)=0$ and as $x$ goes to positive infinity, $f(x)=1$.\n",
    "$$f = \\frac{1}{1+e^{-t}}$$\n",
    "<br/><br/>\n",
    "- Log Function: Used only when $x > 0$ for all $x$.\n",
    "$$ f = log(x) $$\n",
    "<br/><br/>\n",
    "- Log Function + 1\n",
    "$$ f = log(x) + 1$$\n",
    "<br/><br/>\n",
    "- Log Function + 1 Normalized\n",
    "$$g =  \\frac{f(X) - min(f(X))}{max(f(X)) - min(f(X))}, f(X) = log(X)$$\n",
    "<br/><br/>\n",
    "- Cube Root: When numbers are too large\n",
    "$$ f = X^{1/3}$$\n",
    "<br/><br/>\n",
    "- Cube Root Normalized\n",
    "$$ g = \\frac{f(X) - min(f(X))}{max(f(X)) - min(f(X))}, f(X) = X^{1/3}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
