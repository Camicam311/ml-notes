{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent (GD)\n",
    "- \"is descent at estimating parametrs\" - (StatsQuest)\n",
    "<br/><br/>\n",
    "- GD only does a few calculations far from the optimal solution and increases the number of calculations near the optimal value. When the slope of the curve is close to zero, increase number of calculations and when slope of the cruve is far from zero, take big steps because you are far from the optimal value.\n",
    "    - It is also important to take the 'optimal' step since taking too big of a step might lead to slope of the curve being far from zero again.\n",
    "<br/><br/>\n",
    "- GD is useful because it can be used in situations where the derivative = 0.\n",
    "<br/><br/>\n",
    "- GD determines the step size by multiplying the slop by a small number called <strong>learning rate</strong>.\n",
    "<br/><br/>\n",
    "- GD stops when the step size is very close to 0 ( $step size~=~slope\\times learning~rate$ ). If $step~size~=~0$, then $slope$ is also very close to 0.\n",
    "    - In practice, max. number of steps $\\geq 1,000$.\n",
    "<br/><br/>\n",
    "- <strong>GD Steps</strong>:\n",
    "    1. Use SSR ($Sum~of~Squared~of~Residuals$, or other loss function) to evaluate how well model fits the data.\n",
    "<br/><br/>\n",
    "    2. Take the gradient of SSR, i.e. derivative of the loss function.\n",
    "        - Take the derivative w.r.t. the intercept and the slopes. \n",
    "<br/><br/>\n",
    "    3. Pick random values for the intercept and the slopes (parameters), i.e. intercept = $k_0$, slopes = $k_1,...k_n$.\n",
    "<br/><br/>\n",
    "    4. Calculate the derivative with the random values we picked from $step~3$ and plug the slopes into step size calculation ($step size~=~slopes\\times learning~rate$).\n",
    "        - In practice, a reasonable learning rate can be determined automatically by starting large and getting smaller each step.\n",
    "<br/><br/>\n",
    "    5. Calculate the new intercept and slopes by plugging in the old intercept and slopes and the step sizes: ($New~parameters~=~Old~parameters~-~step~sizes$).\n",
    "<br/><br/>\n",
    "    6. Plug the $New~intercept$ and $New~slopes$ into derivative and repeat the process until you get $step~size$ close to $0$ or you reach the max. number of steps.\n",
    "<br/><br/>\n",
    "- Motivation for <strong>Stochastic Gradient Descent</strong>:\n",
    "    - GD is slow for big data.\n",
    "    - When there are millions of data, GD can take longer periods of time, i.e. 10,000 derivatives.\n",
    "    - Stochastic Gradient Descent uses randomly selected subset of the data at every step rather than the full dataset.\n",
    "    - This reduces the time spent calculating the derivatives of the loss function.\n",
    "<br/><br/>\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "- Especially useful when there are redundancies in the data, i.e. clusters of data near each other. So SGD would randomly pick one point from the clusters and calculate.\n",
    "<br/><br/>\n",
    "- Just like GD, SGD is also sensitive to the value of learning rate.\n",
    "<br/><br/>\n",
    "- <strong>Schedule</strong>: The way the learning rate changes, from relatively large to relatively small.\n",
    "<br/><br/>\n",
    "- Strict definition of SGD is to only use 1 sample per step, however it is more common to choose a small subset of data, i.e. <strong>mini-batch</strong>, for each step.\n",
    "    - Using a mini-batch for each step takes the best of both worlds between using just one sample and all of the data at each step. Using a mini-batch can result in more stable estimates of the parameters in fewer steps. Using mini-batch is much faster than using all of the data.\n",
    "<br/><br/>\n",
    "- Another good thing about SGD is that when we have a new data point, we can use it to take another step for the parameter estimates without having to start from scratch. \n",
    "<br/><br/>\n",
    "\n",
    "Sources:\n",
    "- https://www.youtube.com/watch?v=sDv4f4s2SB8&t=203s\n",
    "- https://www.youtube.com/watch?v=vMh0zPT0tLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
