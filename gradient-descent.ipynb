{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent (GD)\n",
    "- <mark>Motivation:</mark> From linear regression, we try to find a line that <strong>best-fits</strong> the data. Similarly, gradient descent finds the <strong>best parameters</strong> that minimizes errors (or residuals).\n",
    "<br/><br/>\n",
    "- \"is descent at estimating parametrs\" - (StatsQuest)\n",
    "<br/><br/>\n",
    "- GD only does a few calculations far from the optimal solution and increases the number of calculations (derivatives of the loss function) near the optimal value. When the slope of the curve is close to zero, increase number of calculations and when slope of the cruve is far from zero, take big steps because you are far from the optimal value.\n",
    "    - It is also important to take the 'optimal' step since taking too big of a step might lead to slope of the curve being far from zero again.\n",
    "<br/><br/>\n",
    "- GD is useful because it can be used in situations where the derivative = 0.\n",
    "<br/><br/>\n",
    "- GD finds the minimum value by taking steps from the initial guess until it reaches the best value whereas in OLS, we simply find where the slope of the derivative of the loss function equals zero.\n",
    "    - <mark>This makes GD very useful when it is not possible to solve for where the derivative = 0, and this is why GD is used in many applications.</mark>\n",
    "    - <mark>Size of the steps GD takes when taking derivatives should be related to the slope, since it tells us if we should take baby steps vs. big steps.</mark>\n",
    "    - Step size is determined by multiplying the slope and the learning rate.\n",
    "<br/><br/>\n",
    "- <mark>GD stops when the step size is very close to 0</mark> ( $step size~=~slope\\times learning~rate$ ). If $step~size~=~0$, then $slope$ is also very close to 0.\n",
    "    - In practice, max. number of steps $\\geq 1,000$.\n",
    "<br/><br/>\n",
    "- <strong>GD Steps</strong>:\n",
    "    1. Use SSR ($Sum~of~Squared~of~Residuals$, or other loss function) to evaluate how well model fits the data.\n",
    "        - <mark>Residual</mark>:Observed value - Predicted value\n",
    "<br/><br/>\n",
    "    2. Take the gradient of SSR, i.e. derivative of the loss function.\n",
    "        - Take the derivative w.r.t. the intercept and the slopes. \n",
    "<br/><br/>\n",
    "    3. Pick random values for the parameters, i.e. intercept = $k_0$, slopes = $k_1,...k_n$ and plug ino equation from part 2 (i.e. gradient).\n",
    "<br/><br/>\n",
    "    4. Plug the slopes into step size calculation ($step size~=~slopes\\times learning~rate$).\n",
    "        - In practice, a reasonable learning rate can be determined automatically by starting large and getting smaller each step.\n",
    "<br/><br/>\n",
    "    5. Calculate the new intercept and slopes by plugging in the old intercept and slopes and the step sizes: ($New~parameters~=~Old~parameters~-~step~sizes$).\n",
    "<br/><br/>\n",
    "    6. Plug the $New~intercept$ and $New~slopes$ into derivative and repeat the process until you get $step~size$ close to $0$ or you reach the max. number of steps.\n",
    "<br/><br/>\n",
    "- Motivation for <strong>Stochastic Gradient Descent</strong>:\n",
    "    - GD is slow for big data.\n",
    "    - When there are millions of data, GD can take longer periods of time, i.e. 10,000 derivatives.\n",
    "    - Stochastic Gradient Descent uses randomly selected subset of the data at every step rather than the full dataset.\n",
    "    - This reduces the time spent calculating the derivatives of the loss function.\n",
    "<br/><br/>\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "- Especially useful when there are redundancies in the data, i.e. clusters of data near each other. So SGD would randomly pick one point from the clusters and calculate.\n",
    "<br/><br/>\n",
    "- Just like GD, SGD is also sensitive to the value of learning rate.\n",
    "<br/><br/>\n",
    "- <strong>Schedule</strong>: The way the learning rate changes, from relatively large to relatively small.\n",
    "<br/><br/>\n",
    "- Strict definition of SGD is to only use 1 sample per step, however it is more common to choose a small subset of data, i.e. <strong>mini-batch</strong>, for each step.\n",
    "    - Using a mini-batch for each step takes the best of both worlds between using just one sample and all of the data at each step. Using a mini-batch can result in more stable estimates of the parameters in fewer steps. Using mini-batch is much faster than using all of the data.\n",
    "<br/><br/>\n",
    "- Another good thing about SGD is that when we have a new data point, we can use it to take another step for the parameter estimates without having to start from scratch. \n",
    "<br/><br/>\n",
    "\n",
    "Sources:\n",
    "- https://www.youtube.com/watch?v=sDv4f4s2SB8&t=203s\n",
    "- https://www.youtube.com/watch?v=vMh0zPT0tLI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
