{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "A <strong>Token</strong> is an instance of a sequence of characters. It is usually part of a bigger document that are grouped together as a semantic unit for processing.\n",
    "\n",
    ">The process of converting sequence of characters into sequence of tokens, i.e. `The cat just entered the room`, will be tokenized into `The`, `cat`, `just`, `entered`, `the`, `room`, where each words are tokens. There is not a distinct way of tokenizing sequence characters. For example, `O'Neil` can be tokenized as `O` and `Neil`, `ONeil` or `O'Neil`. Sometimes we require a language-specific knowledge to tokenize different kinds of characters.\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "<strong>Morpheme</strong> is the smallest unit that makes up words. Consists of two things, namely the <strong>stem</strong> and the <strong>affix</strong>, i.e. given the words `stems` and `affixes`, the stem and affixes are `stem`, `affix` and `s`,`es` accordingly.\n",
    "\n",
    "<strong>Lemmatization</strong>: given a word, we find the correct dictionary form of a given word. It reduces inflection and variant forms to base form.\n",
    "\n",
    ">*Stemming is basically removing the suffix of the word and transforming it to its original form, i.e. the word 'flying' has its original form as 'fly.*\n",
    "\n",
    "<strong>Stemming</strong> refers to a crude heuristic process that chops off affixes of words. In other words, returns only the stems of words. For example, `deliberately`,`deliberation` would be stemmed into `deliberate`.\n",
    "\n",
    ">The goal of stemming and lemmatization is to reduce inflectional forms (inflection: change form of a word) and sometimes derivationally related forms of a word to a common base form. For example, given the following words, `am`,`are`,`is`, we want to categorize them as `be`. More realistically, given `the boy's cars are different colors`, this will be converted to `the boy car be differ color`.\n",
    "\n",
    "- https://www.youtube.com/watch?v=ZhyLgPnOeh0\n",
    "- https://github.com/ShuaiW/data-science-question-answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N gram Model\n",
    "<strong>N gram</strong> is a similar sequence of n items from a given sample of text.\n",
    "\n",
    "<strong>Motivation</strong>: we want to assign probabilities to sentences (between words `high` and `large`, it would be more appropriate to use when we are comparing the size of animals, so the probability on the word `large` will be higher).\n",
    "\n",
    "<strong>Goal</strong>: Calculate probability of sentence or sequence of words:\n",
    "<br/><br/>\n",
    "$$P(W) = P(w_1,w_2,...,w_n) = P(my,name,is,...,etc)$$\n",
    "<br/><br/>\n",
    "<strong>Language model (LM)</strong> is a model that calculates tasks such as the following:\n",
    "<br/><br/>\n",
    "$$P(w_5|w_1,w_2,...,w_4)$$\n",
    "<br/><br/>\n",
    "An example of more complex model is the following (i.e. joint probability of words in sentence):\n",
    "<br/><br/>\n",
    "$$P(Data\\,science\\,is\\,cool) = P(Data|science)\\times P(is|Data\\,science)\\times P(cool|Data\\,Science\\,is)$$\n",
    "<br/><br/>\n",
    "$$=P(w_1,w_2,...,w_n)=\\prod_iP(w_i|w_1w_2...w_{i-a})$$\n",
    "> We estimate these probabilities using the <strong>Markov Assumption</strong>\n",
    "\n",
    "<br/><br/>\n",
    "<strong>Unigram Model</strong>:\n",
    "<br/><br/>\n",
    "$$P(w_1,w_2,...,w_n)\\approx\\prod_iP(w_i)$$\n",
    "<br/><br/>\n",
    "<strong>Bigram Model</strong>:\n",
    "<br/><br/>\n",
    "$$P(w_i|w_1w_2...w_{i-1})\\approx\\prod_iP(w_i|w_{i-1})$$\n",
    "<br/><br/>\n",
    "> As we increase 'n' from the language model, we have a result that looks more like a sentence (i.e. trigrams, 4grams,...,n-grams).\n",
    "\n",
    "> N-gram models are insufficient due to the existence of <strong>long-distance dependencies</strong>. This means that we are only limited to relying on the word right before, from the Markov assumption. It could be that the word at the beginning of the sentence might be key in predicting what the next word will be. etc...\n",
    "\n",
    "> However, in most cases, we are able to get away from these shortcomings just from trigrams or 4-grams. Moreover, trigram is a common choice with large training corpora, whereas a bigram is often used with smaller datasets.\n",
    "\n",
    "<strong>Speech corpus</strong> is a database of speech audio files and text transcriptions.\n",
    "\n",
    "<br/><br/>\n",
    "- https://en.wikipedia.org/wiki/N-gram\n",
    "- https://www.youtube.com/watch?v=hB2ShMlwTyc&list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv&index=12\n",
    "- https://en.wikipedia.org/wiki/Speech_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)\n",
    "<strong>Bag-of-Words (BoW)</strong> is a document represented as a vector of word counts. It is a simplifying representation used in natural language processing and information retrieval (IR). A text is represented as the 'bag' of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "- i.e. \"blue house\" -> (red,blue,house) -> (0,1,1)\n",
    "- i.e. \"red house\" -> (red,blue,house) -> (1,0,1)\n",
    "\n",
    "In other words, BoW builds a vocabulary of all unique words in our dataset and associates unique index to each word in the vocabulary. It's called a 'bag' because it ignores order of words.\n",
    "\n",
    "> Consider this example of two sentences: (1) John likes to watch movies, especially horor movies., (2) Mary likes movies too. We would first build a vocabulary of unique words (all lower cases and ignoring punctuations): [john, likes, to, watch, movies, especially, horor, mary, too]. Then we can represent each sentence using term frequency, i.e, the number of times a term appears. So (1) would be [1, 1, 1, 1, 2, 1, 1, 0, 0], and (2) would be [0, 1, 0, 0, 1, 0, 0, 1, 1].\n",
    "\n",
    ">As we have more vocabularies added in our BoW model, the matrix becomes more sparse.\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "- https://github.com/ShuaiW/data-science-question-answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "<strong>Word Embeddings</strong> is a collective name for a set of language modelling and feature engineering techniques where words or phrases from the vocabulary are mapped to vectors of numbers.\n",
    "\n",
    "<strong>Word2vec</strong> is a group of related models that are used to create word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. It takes a large corpus of text as input and outputs a vector space, with each unique word in the corpus assigned to a corresponding vector in the space. Word vectors are positioned together such that words with similar meaning are located closer to each other in the corpus.\n",
    "\n",
    "Consists of two model architectures: CBoW (Continuous BoW) and Continuous skip-gram\n",
    "- Skip-gram is a language model wherein the components do not have to be consecutive, but rather can skip some components.\n",
    "\n",
    "<strong>Encoding</strong>: Converting text to a number.\n",
    "\n",
    "<strong>One-Hot-Encoding</strong>: Turning texts into vectors.\n",
    "\n",
    "<br/><br/>\n",
    "Sources:\n",
    "- https://github.com/ShuaiW/data-science-question-answer\n",
    "- https://en.wikipedia.org/wiki/Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
